# Installation Ollama - Assistant AI GRATUIT pour ComptaBE

## Pourquoi Ollama?

‚úÖ **100% GRATUIT** - Aucun co√ªt d'API
‚úÖ **Pas de limite** - Utilisez autant que vous voulez
‚úÖ **Confidentialit√© totale** - Tout fonctionne localement
‚úÖ **Rapide** - Mod√®les optimis√©s pour votre machine
‚úÖ **Offline** - Fonctionne sans connexion internet

**Comparaison avec Claude:**
- **Claude:** ~$0.045 par conversation (10 messages) = **$225/mois** pour 100 utilisateurs actifs
- **Ollama:** $0 pour toujours ‚ú®

---

## Installation Rapide (5 minutes)

### √âtape 1: T√©l√©charger Ollama

**Windows:**
1. Allez sur https://ollama.com/download
2. Cliquez sur "Download for Windows"
3. Ex√©cutez le fichier `OllamaSetup.exe`
4. Suivez l'assistant d'installation

**Mac:**
```bash
brew install ollama
```

**Linux:**
```bash
curl -fsSL https://ollama.com/install.sh | sh
```

### √âtape 2: D√©marrer Ollama

Ollama d√©marre automatiquement apr√®s l'installation.

Pour v√©rifier qu'il fonctionne:
```bash
ollama --version
```

Vous devriez voir: `ollama version 0.x.x`

### √âtape 3: T√©l√©charger un mod√®le

Commencez avec **Llama 3.1** (recommand√© - bon √©quilibre vitesse/qualit√©):

```bash
ollama pull llama3.1
```

**Autres mod√®les disponibles:**

| Mod√®le | Taille | Vitesse | Qualit√© | Usage RAM | Recommandation |
|--------|--------|---------|---------|-----------|----------------|
| **llama3.1** | 4.7GB | ‚ö°‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê | 8GB | **Production (recommand√©)** |
| **mistral** | 4.1GB | ‚ö°‚ö°‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê | 6GB | Serveur modeste |
| **phi3** | 2.3GB | ‚ö°‚ö°‚ö°‚ö°‚ö° | ‚≠ê‚≠ê | 4GB | Laptop/d√©veloppement |
| **llama3.1:70b** | 40GB | ‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | 64GB | Serveur puissant |
| **qwen2.5** | 4.7GB | ‚ö°‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê | 8GB | Excellent pour le code |

**Pour t√©l√©charger un autre mod√®le:**
```bash
ollama pull mistral
ollama pull phi3
```

**Pour voir les mod√®les install√©s:**
```bash
ollama list
```

### √âtape 4: Tester Ollama

Testez le mod√®le en ligne de commande:

```bash
ollama run llama3.1
```

Tapez quelque chose comme "Bonjour, comment √ßa va?" et v√©rifiez que le mod√®le r√©pond.

Pour quitter: tapez `/bye`

### √âtape 5: Configuration ComptaBE

Le fichier `.env` est d√©j√† configur√© pour utiliser Ollama:

```bash
# AI Provider Configuration
AI_PROVIDER=ollama

# Ollama Configuration (FREE - Local LLM)
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3.1
OLLAMA_MAX_TOKENS=4096
OLLAMA_TEMPERATURE=0.7
```

**Si vous utilisez un autre mod√®le**, changez `OLLAMA_MODEL`:
```bash
OLLAMA_MODEL=mistral
# ou
OLLAMA_MODEL=phi3
```

### √âtape 6: V√©rifier que tout fonctionne

Dans votre terminal Laravel:

```bash
php artisan tinker
```

Puis testez:
```php
$factory = new \App\Services\AI\AIServiceFactory();
$providers = $factory->getAvailableProviders();
dd($providers);
```

Vous devriez voir:
```php
[
  "ollama" => [
    "name" => "ollama",
    "available" => true,  // ‚úÖ Doit √™tre true!
    "cost" => "Free",
  ],
  "claude" => [
    "name" => "claude",
    "available" => false,
    "cost" => "Paid",
  ],
]
```

---

## Test de l'Assistant AI

1. Connectez-vous √† ComptaBE
2. Cliquez sur l'ic√¥ne de chat en bas √† droite
3. Tapez: **"Bonjour, quelles sont mes factures impay√©es?"**
4. Le bot devrait r√©pondre (avec Ollama, gratuit!)

---

## Configuration avanc√©e

### Changer de mod√®le selon l'environnement

**.env.local (d√©veloppement):**
```bash
OLLAMA_MODEL=phi3  # Rapide pour dev
```

**.env.production:**
```bash
OLLAMA_MODEL=llama3.1  # Meilleure qualit√©
```

### Utiliser un serveur Ollama distant

Si Ollama tourne sur un autre serveur:

```bash
OLLAMA_BASE_URL=http://192.168.1.100:11434
```

### Augmenter la vitesse

Pour des r√©ponses plus rapides (moins pr√©cises):
```bash
OLLAMA_TEMPERATURE=0.3
OLLAMA_MAX_TOKENS=2048
```

### Basculer vers Claude temporairement

Si vous voulez tester Claude:
```bash
AI_PROVIDER=claude
CLAUDE_API_KEY=sk-ant-api03-votre-cl√©
```

Puis revenez √† Ollama:
```bash
AI_PROVIDER=ollama
```

---

## D√©pannage

### Erreur: "Ollama not available"

**V√©rifiez qu'Ollama tourne:**
```bash
curl http://localhost:11434/api/tags
```

Si erreur, d√©marrez Ollama:
- **Windows:** Ouvrir "Ollama" depuis le menu D√©marrer
- **Mac/Linux:** `ollama serve`

### Mod√®le non trouv√©

```bash
# V√©rifier mod√®les install√©s
ollama list

# T√©l√©charger le mod√®le manquant
ollama pull llama3.1
```

### R√©ponses lentes

**Options:**
1. Utilisez un mod√®le plus l√©ger: `phi3` ou `mistral`
2. Ajoutez plus de RAM √† votre serveur
3. Utilisez un GPU (NVIDIA/AMD) - Ollama l'utilisera automatiquement

### Utilisation CPU/RAM √©lev√©e

C'est normal! Les mod√®les LLM utilisent beaucoup de ressources.

**Solutions:**
- Mod√®le plus petit: `phi3` (2.3GB RAM)
- Limitez conversations simultan√©es
- Ajoutez un cache devant Ollama

### R√©ponses de mauvaise qualit√©

**Essayez:**
1. Mod√®le plus gros: `llama3.1:70b` (si vous avez 64GB RAM)
2. Augmenter temp√©rature: `OLLAMA_TEMPERATURE=0.9`
3. Basculer vers Claude pour t√¢ches critiques

---

## Comparaison des co√ªts (100 utilisateurs, 50 conv/mois)

| Provider | Co√ªt mensuel | Co√ªt annuel |
|----------|--------------|-------------|
| **Ollama (llama3.1)** | **$0** ‚ú® | **$0** ‚ú® |
| Claude Sonnet | $225 | $2,700 |
| GPT-4 | $300 | $3,600 |

**Ollama = √âconomie de $2,700/an minimum!**

---

## Maintenance

### Mettre √† jour Ollama

**Windows:** T√©l√©charger la nouvelle version depuis le site

**Mac:**
```bash
brew upgrade ollama
```

**Linux:**
```bash
curl -fsSL https://ollama.com/install.sh | sh
```

### Mettre √† jour un mod√®le

```bash
ollama pull llama3.1
```

### Supprimer un mod√®le

```bash
ollama rm phi3
```

### Lib√©rer de l'espace

```bash
# Voir les mod√®les
ollama list

# Supprimer les mod√®les inutilis√©s
ollama rm nom-du-modele
```

---

## FAQ

**Q: Ollama est-il vraiment gratuit?**
A: Oui, 100% gratuit et open source. Pas de limites, pas de co√ªts cach√©s.

**Q: Puis-je utiliser Ollama en production?**
A: Absolument! Des milliers d'entreprises l'utilisent. Assurez-vous d'avoir assez de RAM.

**Q: Ollama est-il moins bon que Claude?**
A: Pour des t√¢ches simples (factures, questions basiques): qualit√© similaire. Pour des t√¢ches complexes: Claude est meilleur mais co√ªte cher.

**Q: Puis-je utiliser les deux (Ollama + Claude)?**
A: Oui! Configurez `AI_PROVIDER=ollama` par d√©faut, et basculez vers Claude pour t√¢ches critiques.

**Q: Combien de RAM n√©cessaire?**
A: Minimum 8GB pour llama3.1. Recommand√© 16GB pour usage fluide.

**Q: Fonctionne avec Docker?**
A: Oui! Image disponible: `docker pull ollama/ollama`

**Q: Supporte GPU?**
A: Oui, automatiquement si NVIDIA/AMD GPU d√©tect√©. 10x plus rapide!

---

## Ressources

- **Site officiel:** https://ollama.com
- **GitHub:** https://github.com/ollama/ollama
- **Mod√®les disponibles:** https://ollama.com/library
- **Documentation:** https://github.com/ollama/ollama/tree/main/docs

---

## Support

Pour toute question:
1. V√©rifiez d'abord ce guide
2. Testez avec `ollama run llama3.1` en CLI
3. Consultez logs Laravel: `storage/logs/laravel.log`

**Status final:** ‚úÖ Ollama est configur√©, ComptaBE utilise d√©sormais un AI **100% gratuit**! üéâ
